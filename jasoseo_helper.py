import os
import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from bs4 import BeautifulSoup
import requests

st.title("ğŸ’¬ ìê¸°ì†Œê°œì„œ ì‘ì„± ë„ìš°ë¯¸")

# 1. API KEY
from dotenv import load_dotenv
load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')

# 2. ê²½ë ¥ê¸°ìˆ ì„œ ì—…ë¡œë“œ
docs = []
uploaded_file = st.file_uploader("ê²½ë ¥ê¸°ìˆ ì„œ ì—…ë¡œë“œ (.pdf/.docx)", type=['pdf','docx'])
if uploaded_file:
    if uploaded_file.name.endswith('.pdf'):
        loader = PyPDFLoader(uploaded_file)
    else:
        loader = Docx2txtLoader(uploaded_file)
    docs = loader.load_and_split()

# 3. JD ì…ë ¥ (URL)
jd_url = st.text_input('JD URL ì…ë ¥')

jd_docs = []
if jd_url:
    try:
        res = requests.get(jd_url, timeout=5)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        text = soup.get_text()
        jd_docs = [Document(page_content=text)]
    except Exception as e:
        st.warning(f"JD URL ë¡œë”© ì‹¤íŒ¨: {e}")

# 4. VectorStore ìƒì„± ì¤€ë¹„
all_docs = docs + jd_docs
if all_docs:
    text_splitter = RecursiveCharacterTextSplitter(
    separators=['\n\n', '\n'],
    chunk_size=500,
    chunk_overlap=100,
    length_function=len,
    is_separator_regex=False
    )

    split_docs = text_splitter.split_documents(all_docs)

    # OpenAI ì„ë² ë”© ëª¨ë¸
    embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')

    # Chroma ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (persist_directory ì§€ì •í•˜ë©´ ì¬ì‚¬ìš© ê°€ëŠ¥)
    persist_dir = './chroma_resume_db'
    vectorstore = Chroma.from_documents(
        documents=split_docs,
        embedding=embeddings_model,
        persist_directory=persist_dir,
        collection_name='resume_seungwoo'
    )

    # Retriever ì¤€ë¹„
    retriever = vectorstore.as_retriever()
    st.success("VectorStore ìƒì„± ì™„ë£Œ âœ…")
else:
    st.info("ì—…ë¡œë“œëœ ê²½ë ¥ê¸°ìˆ ì„œ ë˜ëŠ” JD URLì´ ì—†ìŠµë‹ˆë‹¤.")
    
# 5. RAG í”„ë¡¬í”„íŠ¸
qa_system_prompt = """
ê²½ë ¥ê¸°ìˆ ì„œì™€ JD ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìê¸°ì†Œê°œì„œ ì§ˆë¬¸ì— ëŒ€í•´ í•œê¸€ë¡œ ë‹µë³€í•˜ì„¸ìš”. 
ì§ˆë¬¸: {input}
ê´€ë ¨ ë‚´ìš©: {context}
"""
qa_prompt = ChatPromptTemplate.from_messages([
    ("system", qa_system_prompt),
    ("human", "{input}")
])
llm = ChatOpenAI(model="gpt-4o-mini")
rag_chain = (
    {"context": retriever | (lambda docs: "\n\n".join([d.page_content for d in docs])),
     "input": RunnablePassthrough()}
    | qa_prompt
    | llm
    | StrOutputParser()
)

# 6. Streamlit chat
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role":"assistant", "content":"ìê¸°ì†Œê°œì„œ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!"}]

for msg in st.session_state["messages"]:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    st.session_state["messages"].append({"role":"user","content":prompt})
    st.chat_message("user").write(prompt)
    
    response = rag_chain.invoke(prompt)
    st.session_state["messages"].append({"role":"assistant","content":response})
    st.chat_message("assistant").write(response)
