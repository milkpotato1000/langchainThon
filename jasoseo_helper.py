import os
from dotenv import load_dotenv
import tempfile
from chromadb import Client
from chromadb.config import Settings
import streamlit as st

from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import MultiRetrievalQAChain


#ì˜¤í”ˆAI API í‚¤ ì„¤ì •
load_dotenv()  # í˜„ì¬ ê²½ë¡œì˜ .env ë¡œë“œ
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')
if not os.environ['OPENAI_API_KEY']:
    raise ValueError('OPENAI_API_KEY not found in environment. set it in .env or env vars')

@st.cache_resource #cache_resourceë¡œ í•œë²ˆ ì‹¤í–‰í•œ ê²°ê³¼ ìºì‹±í•´ë‘ê¸°
# CV ë¶ˆëŸ¬ì˜¤ê¸° (PDF/Word)
def load_cv(_file):
    with tempfile.NamedTemporaryFile(mode='wb', delete=False) as tmp_file:
        tmp_file.write(_file.getvalue())
        tmp_file_path = tmp_file.name
        
    #PDF íŒŒì¼ ì—…ë¡œë“œ
    if _file.name.endswith('.pdf'):
        loader = PyPDFLoader(file_path=tmp_file_path)
    
    #Word íŒŒì¼ ì—…ë¡œë“œ
    elif _file.name.endswith('.docx'):
        loader = Docx2txtLoader(file_path=tmp_file_path)
    
    # íŒŒì¼ í˜•ì‹ì´ í‹€ë¦´ê²½ìš° ì—ëŸ¬ ë©”ì„¸ì§€ ì¶œë ¥
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. PDF ë˜ëŠ” DOCXë§Œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    
    pages = loader.load()
    return pages

@st.cache_resource
# JD ë¶ˆëŸ¬ì˜¤ê¸° (URL)
def load_jd(_url: str):
    import warnings
    from urllib3.exceptions import InsecureRequestWarning
    warnings.simplefilter('ignore', InsecureRequestWarning)
    
    loader = WebBaseLoader(_url)
    loader.requests_kwargs = {'verify': False}  # SSL ê²€ì¦ ë¹„í™œì„±í™”
    return loader.load()


@st.cache_resource
#ë¬¸ì„œ ë‚˜ëˆ„ê¸°
def chunk_documents(docs, source_label, chunk_size=500, chunk_overlap=100):
    '''
    í˜ì´ì§€(Document list)ë“¤ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ê³  metadataë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜
    - docs: Document list
    - source_label: 'cv' ë˜ëŠ” 'jd' ë“± source í‘œì‹œ
    - return: ì²­í¬ê°€ ë‚˜ë‰œ Document ë¦¬ìŠ¤íŠ¸
    '''
    text_splitter = RecursiveCharacterTextSplitter(
        separators=['\n\n', '\n', ' '],
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        is_separator_regex=False,
    )

    split_docs = text_splitter.split_documents(docs)

    split_docs_with_meta = [
        Document(page_content=d.page_content,
                 metadata={'source': source_label, 'chunk_id': str(i)},)
        for i, d in enumerate(split_docs)
    ]

    return split_docs_with_meta

@st.cache_resource
#í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ Chroma ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥
def create_vector_store(_cv, _jd):
    '''
    CV, JD ë¬¸ì„œë¥¼ Chroma ë²¡í„°ìŠ¤í† ì–´ë¡œ ë³€í™˜
    - st.cache_resource ì œê±° â†’ ë°˜ë³µ í˜¸ì¶œí•´ë„ ëˆ„ì  ë¬¸ì œ ì—†ìŒ
    '''
    # ì²­í¬ ìƒì„±
    cv_docs = chunk_documents(_cv, 'cv')
    jd_docs = chunk_documents(_jd, 'jd')
    
    # í´ë¼ì´ì–¸íŠ¸ ë° ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    client_chroma = Client(Settings(is_persistent=False))
    embeddings  = OpenAIEmbeddings(model="text-embedding-3-small")

    vectorstore = Chroma.from_documents(
        documents=cv_docs+jd_docs,
        embedding=embeddings,
        client=client_chroma,
    )

    return vectorstore

@st.cache_resource
#RAG ì²´ì¸ êµ¬ì¶•
def chaining(_cv, _jd):
    vectorstore = create_vector_store(_cv, _jd)
    
    # -----------------------------
    # Retriever ì„¤ì •
    # -----------------------------
    # CV Retriever
    cv_retriever = vectorstore.as_retriever(
        search_type='mmr',
        search_kwargs={'k': 5, 'filter': {'source': 'cv'}}
    )
    # JD Retriever
    jd_retriever = vectorstore.as_retriever(
        search_type='mmr',
        search_kwargs={'k': 5, 'filter': {'source': 'jd'}}
    )
    # Default Retriever
    default_retriever = vectorstore.as_retriever(
        search_type='mmr',
        search_kwargs={'k': 6}
    )

    # -----------------------------
    # System prompt ìƒì„±
    # -----------------------------
    system_prompt = '''
    ë‹¹ì‹ ì€ ì»¤ë¦¬ì–´ ë° ì±„ìš© ê´€ë ¨ ì§ˆì˜ì‘ë‹µì„ ë•ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.  
    ì•„ë˜ ì œê³µëœ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.  

    - ì‚¬ìš©ìì˜ **ê²½ë ¥, ê²½í—˜, ì´ë ¥ ê´€ë ¨ ì§ˆë¬¸**ì´ë¼ë©´ CV(ì´ë ¥ì„œ)ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•˜ì„¸ìš”.  
    - **ì§€ì›í•˜ëŠ” íšŒì‚¬, ì§ë¬´, ì±„ìš© ìš”ê±´** ê´€ë ¨ ì§ˆë¬¸ì´ë¼ë©´ JD(ê³µê³ ë¬¸)ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•˜ì„¸ìš”.  
    - ì§ˆë¬¸ì˜ ë§¥ë½ìƒ ë‘ ë¬¸ì„œê°€ ëª¨ë‘ ê´€ë ¨ë˜ì–´ ìˆë‹¤ë©´, **ê· í˜• ìˆê²Œ í†µí•©í•˜ì—¬ ë‹µë³€**í•˜ì„¸ìš”.  
    - ë¬¸ì„œì— ì§ì ‘ì ì¸ ì •ë³´ê°€ ì—†ì„ ê²½ìš°, ì¼ë°˜ì ì¸ HR/ì»¤ë¦¬ì–´ ìƒì‹ê³¼ ë…¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³´ì™„í•´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ìê¸°ì†Œê°œì„œ ì‘ì„± ë˜ëŠ” ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ìš”ì²­ì‹œ ê°ê° ì•„ë˜ì™€ ê°™ì€ ì§€ì¹¨ì„ ë”°ë¦…ë‹ˆë‹¤.

    ** ìê¸°ì†Œê°œì„œ ì‘ì„± ì§€ì¹¨**
    - ìì—°ìŠ¤ëŸ½ê³  ì–µì§€ìŠ¤ëŸ½ì§€ ì•Šì€ ë¬¸ì¥ìœ¼ë¡œ ê¼­ í•„ìš”í•œ ë¶€ë¶„ì—ë§Œ CV ë‚´ìš©ì„ í™œìš©í•©ë‹ˆë‹¤.
    - ê²½í—˜ê³¼ ê´€ë ¨ëœ ì‚¬ë¡€ë¥¼ ë“¤ì–´ì•¼ í•  ê²½ìš° ë°˜ë“œì‹œ CVì— ê¸°ìˆ ëœ ë‚´ìš©ë§Œì„ ì‚¬ì‹¤ëŒ€ë¡œ ë§í•©ë‹ˆë‹¤.
    - ì‘ì„±ëœ ê²½í—˜ë“¤ì„ ì ì ˆí•˜ê²Œ JDì—ì„œ ìš”êµ¬í•˜ëŠ” ëŠ¥ë ¥ ë° ì·¨ì—… í›„ ì„±ê³¼ ê¸°ì—¬ê°€ ê°€ëŠ¥í•¨ì˜ ê·¼ê±°ë¡œì¨ í™œìš©í•©ë‹ˆë‹¤.
    - ìê¸°ì†Œê°œì„œ ì‘ì„±ì‹œ ê¸€ììˆ˜ ì œí•œì€ í•œê¸€ì„ ê¸°ì¤€ìœ¼ë¡œ ë„ì–´ì“°ê¸°ë¥¼ í¬í•¨í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.
    - ì§ˆë¬¸ì§€ì— ì§ˆë¬¸ì˜ í•­ëª© ë˜ëŠ” ë²ˆí˜¸ê°€ ìˆë”ë¼ë„, ë¬¸ì¥ í˜•íƒœë¡œ ë‹µì„ ì‘ì„±í•©ë‹ˆë‹¤.

    ** ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ì§€ì¹¨**
    - íšŒì‚¬ JDë¥¼ ì°¸ê³ í•˜ì—¬ ì§€ì›ìì˜ ëŠ¥ë ¥ì„ í‰ê°€í•  ìˆ˜ ìˆì„ë§Œí•œ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - ì¼ë°˜ì ì¸ HR/ì»¤ë¦¬ì–´ ìƒì‹ê³¼ ë…¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„± í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    **ì–¸ì–´ ìŠ¤íƒ€ì¼ ì§€ì¹¨**
    - ë‹µë³€ì€ ì–¸ì œë‚˜ **ì „ë¬¸ì ì´ê³  ì‹ ë¢°ê° ìˆëŠ” ì–´íˆ¬**ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.  
    - ë¬¸ì¥ì€ ëª…ë£Œí•˜ê³  ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ë˜, ë¹„ì¦ˆë‹ˆìŠ¤ ìƒí™©ì— ë§ëŠ” ì ì ˆí•œ ì–´íœ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.  
    - í•„ìš” ì‹œ ê°€ë²¼ìš´ ì´ëª¨ì§€ë¥¼ í™œìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì¹œê·¼í•¨ì„ ë”í•  ìˆ˜ ìˆìœ¼ë‚˜, ê³¼ë„í•˜ê²Œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  
        
    ë¬¸ì„œ ë‚´ìš©:
    {context}
    '''

    # -----------------------------
    # ChatPromptTemplate ìƒì„±
    # -----------------------------
    qa_prompt = ChatPromptTemplate.from_messages([
        ('system', system_prompt),
        ('human', '{question}')
    ])


    # -----------------------------
    # MultiRetrievalQAChain êµ¬ì„±
    # -----------------------------
    retriever_infos = [
        {
            'name': 'cv',
            'description': 'userì˜ ê²½ë ¥(CV) ë° ì´ì „ íšŒì‚¬ ê´€ë ¨ ì§ˆë¬¸',
            'retriever': cv_retriever,
            'prompt': qa_prompt
        },
        {
            'name': 'jd',
            'description': 'ì§€ì›í•˜ëŠ” íšŒì‚¬ ë° ì§ë¬´ê¸°ìˆ ì„œ(JD) ê´€ë ¨ ì§ˆë¬¸',
            'retriever': jd_retriever,
            'prompt': qa_prompt
        },
    ]

    llm = ChatOpenAI(model='gpt-4o-mini', temperature=0.4)

    rag_chain = MultiRetrievalQAChain.from_retrievers(
        llm=llm,
        retriever_infos=retriever_infos,
        default_retriever=default_retriever,
    )

    return rag_chain


# -----------------------------
# Streamlit UI
# -----------------------------

# CV íŒŒì¼ ì—…ë¡œë“œ
st.header('ê²½ë ¥ê¸°ìˆ ì„œ(CV) & JD URL ì…ë ¥ ğŸ’¬ ğŸ“š ')
cv_file = st.file_uploader('ê²½ë ¥ê¸°ìˆ ì„œ(.pdf/.docx)ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.', type=['pdf', 'docx'])
jd_url = st.text_input('JD(URL)ì„ ì…ë ¥í•˜ì„¸ìš”.', placeholder='ì˜ˆ: https://example.com/job_description')

if cv_file is not None and jd_url:
    st.success('CVì™€ JDê°€ ëª¨ë‘ ì—…ë¡œë“œ/ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤. RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.')
    
    cv = load_cv(cv_file)
    jd = load_jd(jd_url)

    rag_chain = chaining(cv, jd)

    if 'messages' not in st.session_state:
        st.session_state['messages'] = [{'role': 'assistant', 'content': 'ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!'}]

    for msg in st.session_state.messages:
        st.chat_message(msg['role']).write(msg['content'])

    if prompt_message := st.chat_input('ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :)'):
        st.chat_message('human').write(prompt_message)
        st.session_state.messages.append({'role': "user", 'content': prompt_message})
        with st.chat_message('ai'):
            with st.spinner('Thinking...'):
                response = rag_chain.invoke(prompt_message)
                st.session_state['messages'].append({'role': 'assistant', 'content': response['result']})
                
                # LLM ë‹µë³€ ì¶œë ¥
                st.markdown('ğŸ§  **GPT-4o-mini ë‹µë³€:**')
                st.write(response['result'])
                
else:
    st.info('ê²½ë ¥ê¸°ìˆ ì„œì™€ JD URLì„ ëª¨ë‘ ì…ë ¥í•´ì•¼ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.')