{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7af3cb2d-a2a3-4678-adb0-aed0935a73d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import tempfile\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad355284-e0cc-4b90-a93e-90d59d654c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chroma tenant 오류 방지 위한 코드\n",
    "import chromadb\n",
    "chromadb.api.client.SharedSystemClient.clear_system_cache()\n",
    "\n",
    "#오픈AI API 키 설정\n",
    "load_dotenv()  # 현재 경로의 .env 로드\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "if not os.environ['OPENAI_API_KEY']:\n",
    "    raise ValueError('OPENAI_API_KEY not found in environment. set it in .env or env vars')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb14d70-0699-4556-bd0a-e6a73e33d360",
   "metadata": {},
   "source": [
    "## 1. 문서 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e182d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ded273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/langchainThon/cv/경력기술서_천승우.docx'\n",
    "# 파일을 바이너리 모드로 읽어서 _file처럼 흉내내기\n",
    "class DummyFile:\n",
    "    def __init__(self, path):\n",
    "        self.name = path\n",
    "        with open(path, \"rb\") as f:\n",
    "            self._data = f.read()\n",
    "    def getvalue(self):\n",
    "        return self._data\n",
    "\n",
    "_file = DummyFile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edb95b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.NamedTemporaryFile(mode='wb', delete=False) as tmp_file:\n",
    "    tmp_file.write(_file.getvalue())\n",
    "    tmp_file_path = tmp_file.name\n",
    "#PDF 파일 업로드\n",
    "if _file.name.endswith('.pdf'):\n",
    "    loader = PyPDFLoader(file_path=tmp_file_path)\n",
    "#Word 파일 업로드\n",
    "elif _file.name.endswith('.docx'):\n",
    "    loader = Docx2txtLoader(file_path=tmp_file_path)\n",
    "else:\n",
    "    raise ValueError(\"지원하지 않는 파일 형식입니다. PDF 또는 DOCX만 업로드해주세요.\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e2c8e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\tmpedt4i9vn'}, page_content='경력기술서\\n\\n[개인 정보]\\n\\n이메일: fourleaves8@gmail.com\\n\\n연락처: 010-4788-7980\\n\\n\\n\\n[전문 분야]\\n\\n• 데이터 사이언스       \\t\\t • AI 엔지니어링\\t\\n\\n• 데이터 분석           \\t\\t • 프롬프트 엔지니어링\\n\\n• 생물분자 및 화학공학 연구\\n\\n\\n\\n[요약]\\n\\n저는 생명·의학 데이터를 처리하고 분석 플랫폼을 구축한 경험 및 복잡한 데이터를 효율적으로 분석해 의사결정을 지원하는 역량을 갖추고 있습니다. Python 기반의 데이터 전처리, 알고리즘 고도화, 분류 및 예측 모델 개발을 통해 항암제 신규 적응증 탐색 플랫폼을 성공적으로 구현한 바 있습니다. 또한 데이터 수집·정규화·시각화 전 과정을 주도하며, 스타트업의 코스닥 상장 및 실제 제약사 프로젝트에 기여한 실무 경험을 쌓았습니다. \\n\\n\\n\\n[보유 역량 및 기술]\\n\\n프로그래밍 언어\\n\\nPython, R, SQL, Java\\n\\n도구 및 프레임워크\\n\\npandas, NumPy, Matplotlib, seaborn, Plotly, scikit-learn, PyTorch, TensorFlow, RAG, LangChain, MySQL, Oracle\\n\\n운영체제\\n\\nLinux(Ubuntu), Windows, macOS\\n\\nPalantir Foundry\\n\\nPipeline Builder, Contour, Ontology(Action), Workshop, AIP, Dataset\\n\\n기타\\n\\nVim / Vi, Git / GitHub, Bioinformatics Tools (Gene Set Enrichment Analysis, Bismark, Metilene, lifelines, survival, survminer, lifelines)\\n\\n\\n\\n[주요 경력 및 프로젝트]\\n\\n(주)온코크로스 · AI 연구소 · 주임연구원–전임연구원 · 2021.01–2024.12 (4년간 근무)\\n\\n\\n\\n<프로젝트1 : 항암제 신규 적응증 탐색 플랫폼 개발 프로젝트>\\n\\n1) 담당 업무 및 역할: 플랫폼 전체 기능 Python으로 재 구현, 내재화 및 오퍼레이션\\n\\n2) 프로젝트 상세 내용\\n\\n프로젝트 기간 : 2021.01–2024.01 (36개월)\\n\\n목표 및 역할 : 플랫폼 개발/운영 · 전사체 데이터 및 의료 메타데이터 관리/분석\\n\\n성과 (개인 기여도: 70%) :\\n\\n플랫폼 전체 기능 Python 구현 및 내재화 완료\\n\\n플랫폼 오퍼레이션 전반에 대한 유지, 보수, 관리 및 분석 프로젝트를 수행함\\n\\n공공 데이터베이스로부터 23개 암종에 걸쳐 약 24,000개 샘플 데이터를 수집하여 대규모 분석 기반을 구축함\\n\\n수집한 샘플 전사체 데이터 정규화 및 정량적 유전자 발현량으로 변환하여 분석 가능한 데이터셋을 구축함\\n\\n플랫폼 대표 성공 사례\\n\\n중국 ㅇㅇㅇ사 개발 신규 항암제의 주 적응증 예측 성공 및 잠재 적응증 후보 추가 제안\\n\\n국내 유명 제약사와 협업 – 후보물질 개발 단계 참여, 개발 물질에 대한 적응증 후보 도출, 최종 약물 타깃으로 선정되어 개발 지속\\n\\n상장 기술평가 항목 3가지 플랫폼 중 하나로서 회사 상장에 기여\\n\\nPython · R · pandas · NumPy · scikit-learn · Bioinformatics tools (survival, survminer, lifelines, GSEA, etc.) · Visualization packages (Matplotlib, seaborn, Plotly, etc.)\\n\\n\\n\\n<프로젝트2 : 항암제 플랫폼 고도화 프로젝트>\\n\\n1) 담당 업무 및 역할 : 암종 별, 유전자 예후 예측 스코어링 알고리즘 고도화\\n\\n2) 프로젝트 상세 내용\\n\\n프로젝트 기간 : 2024.01–2024.06 (6개월)\\n\\n목표 및 역할 : 23개 암종에 대한 유전자 별 예후 점수 스코어링 알고리즘 속도 2배 (100%) 이상 개선\\n\\n성과 (개인 기여도: 100%) : \\n\\n코어 스코어링 알고리즘 전면 수정\\n\\n알고리즘 스코어링 속도 1100% 향상\\n\\n알고리즘 개선 후 소요시간: 최대 1주일 (기존 알고리즘: 최소 3개월)\\n\\nPython · pandas · NumPy · scikit-learn · Bioinformatics tools (lifelines, GSEA, etc.) · Visualization packages (Matplotlib, seaborn, Plotly, etc.)\\n\\n\\n\\n<프로젝트3 : 엑체생검을 통한 암 조기 진단 모델 개발>\\n\\n1) 담당 업무 및 역할 : \\n\\ncfDNA, ctDNA 데이터 수집, 관리, 분석 \\n\\n메타데이터 관리 \\n\\n암 진단 딥러닝 모델 개발\\n\\n2) 프로젝트 상세 내용\\n\\n프로젝트 기간 : 2025.05–2025.11 (6개월)\\n\\n목표 및 역할 : 혈액 cfDNA, ctDNA 데이터 수집 및 활용, 암 여부 예측 딥러닝 모델 개발\\n\\n성과 (개인 기여도: 20%) : \\n\\n모델 훈련을 위한 데이터 수집 (3000샘플 이상)\\n\\n암 진단 CNN모델 개발 (Recall: 0.69, Precision: 0.64, ROC-AUC: 0.71)\\n\\nPython · pandas · NumPy · PyTorch, TensorFlow · scikit-learn · Bioinformatics tools (GSEA, Bismark, Metilene) · Visualization packages (Matplotlib, seaborn, Plotly, etc.)\\n\\n\\n\\n<프로젝트4 : 신규 COVID-19 치료 후보 약물 탐색 및 연구 결과 논문 게재>\\n\\n1) 담당 업무 및 역할 : Dry-lab 연구, 내용 정리 및 논문 작성\\n\\n2) 프로젝트 상세 내용\\n\\n프로젝트 기간 : 2024.01–present (21개월)\\n\\n목표 및 역할 : 공동 1저자(주 저자), 논문 Submission (2024.05.09)\\n\\n성과 (개인 기여도: 40%) : \\n\\n사내 플랫폼 활용 COVID-19 치료 유력 후보물질 발굴 성공\\n\\n연구 결과 Scientific reports 저널에 제출\\n\\n2025년 9월 현재까지 revision 진행중 (in press)\\n\\nPython · pandas · NumPy · scikit-learn · GSEA · Visualization packages (Matplotlib, seaborn)\\n\\n\\n\\n[자격증]\\n\\nOPIc(영어) · AL (Advanced Low) · 2024.09.20\\n\\n데이터 분석 준전문가 (ADsP) · 2025.09.05\\n\\n\\n\\n[교육 수료]\\n\\nGoogle Advanced Data Analytics – Foundations of Data Science\\n\\nGoogle Advanced Data Analytics – Get Started with Python\\n\\nGoogle Advanced Data Analytics – Go Beyond the Numbers: Translate Data into Insights\\n\\nGoogle Advanced Data Analytics – The Power of Statistics\\n\\nGoogle Advanced Data Analytics – Regression Analysis: Simplify Complex Data Relationships\\n\\nGoogle Advanced Data Analytics – The Nuts and Bolts of Machine Learning\\n\\nPalantir Foundry – Speedrun: Your First End-to-End Workflow\\n\\nPalantir Foundry – Speedrun: Your First AIP workflow\\n\\nPalantir Foundry – Deep Dive: Building Your First Pipeline\\n\\nPalantir Foundry – Deep Dive: Creating Your First Ontology\\n\\nPalantir Foundry – Deep Dive: Transforming Your Data with Code Repositories\\n\\nPalantir Foundry – Deep Dive: Building Your First Application\\n\\nPalantir Foundry – Deep Dive: Data Analysis in Contour\\n\\n모두의 연구소 – 데이터 사이언티스트 과정 Bootcamp (2025.06~현재 진행중)\\n\\n\\n\\n[학력]\\n\\n학위\\n\\n전공\\n\\n학교\\n\\n기간\\n\\n박사 (수료)\\n\\n생명화학공학\\n\\n한국과학기술원 (KAIST, 대전)\\n\\n2015.08 – 2020.09\\n\\n석사 (졸업)\\n\\n생명화학공학\\n\\n한국과학기술원 (KAIST, 대전)\\n\\n2015.08 – 2020.09\\n\\n학사 (졸업)\\n\\n생명과학\\n\\nVirginia Tech (미국)\\n\\n2011.08\\n\\n\\n\\n[기타]\\n\\nGithub Personal Project Repository\\n\\n(https://github.com/milkpotato1000/modulabs_projects)\\n\\nData Science & AI학습 블로그 \\n\\n(https://velog.io/@milkpotato1000/posts)\\n\\n\\n\\n\\n\\n     2')]\n"
     ]
    }
   ],
   "source": [
    "print(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e03f9c-6738-40bf-8394-4d9896379b8c",
   "metadata": {},
   "source": [
    "## 2. 텍스트 나누기, 임베딩, 벡터DB 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff30dc7f-93bb-4f0f-8245-8e6de760ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 나누기\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=['\\n\\n', '\\n'],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "split_docs = text_splitter.split_documents(pages)\n",
    "\n",
    "split_docs_with_chunk_id = []\n",
    "for i, doc in enumerate(split_docs):\n",
    "    new_meta = doc.metadata.copy()  # 기존 metadata 보존\n",
    "    new_meta['chunk_id'] = str(i)\n",
    "    split_docs_with_chunk_id.append(\n",
    "        Document(page_content=doc.page_content, metadata=new_meta)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656d2712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\tmpedt4i9vn', 'chunk_id': '0'}, page_content='경력기술서\\n\\n[개인 정보]\\n\\n이메일: fourleaves8@gmail.com\\n\\n연락처: 010-4788-7980\\n\\n\\n\\n[전문 분야]\\n\\n• 데이터 사이언스       \\t\\t • AI 엔지니어링\\t\\n\\n• 데이터 분석           \\t\\t • 프롬프트 엔지니어링\\n\\n• 생물분자 및 화학공학 연구\\n\\n\\n\\n[요약]\\n\\n저는 생명·의학 데이터를 처리하고 분석 플랫폼을 구축한 경험 및 복잡한 데이터를 효율적으로 분석해 의사결정을 지원하는 역량을 갖추고 있습니다. Python 기반의 데이터 전처리, 알고리즘 고도화, 분류 및 예측 모델 개발을 통해 항암제 신규 적응증 탐색 플랫폼을 성공적으로 구현한 바 있습니다. 또한 데이터 수집·정규화·시각화 전 과정을 주도하며, 스타트업의 코스닥 상장 및 실제 제약사 프로젝트에 기여한 실무 경험을 쌓았습니다. \\n\\n\\n\\n[보유 역량 및 기술]\\n\\n프로그래밍 언어\\n\\nPython, R, SQL, Java\\n\\n도구 및 프레임워크\\n\\npandas, NumPy, Matplotlib, seaborn, Plotly, scikit-learn, PyTorch, TensorFlow, RAG, LangChain, MySQL, Oracle\\n\\n운영체제\\n\\nLinux(Ubuntu), Windows, macOS\\n\\nPalantir Foundry\\n\\nPipeline Builder, Contour, Ontology(Action), Workshop, AIP, Dataset\\n\\n기타\\n\\nVim / Vi, Git / GitHub, Bioinformatics Tools (Gene Set Enrichment Analysis, Bismark, Metilene, lifelines, survival, survminer, lifelines)\\n\\n\\n\\n[주요 경력 및 프로젝트]\\n\\n(주)온코크로스 · AI 연구소 · 주임연구원–전임연구원 · 2021.01–2024.12 (4년간 근무)\\n\\n\\n\\n<프로젝트1 : 항암제 신규 적응증 탐색 플랫폼 개발 프로젝트>'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\tmpedt4i9vn', 'chunk_id': '1'}, page_content='(주)온코크로스 · AI 연구소 · 주임연구원–전임연구원 · 2021.01–2024.12 (4년간 근무)\\n\\n\\n\\n<프로젝트1 : 항암제 신규 적응증 탐색 플랫폼 개발 프로젝트>\\n\\n1) 담당 업무 및 역할: 플랫폼 전체 기능 Python으로 재 구현, 내재화 및 오퍼레이션\\n\\n2) 프로젝트 상세 내용\\n\\n프로젝트 기간 : 2021.01–2024.01 (36개월)\\n\\n목표 및 역할 : 플랫폼 개발/운영 · 전사체 데이터 및 의료 메타데이터 관리/분석\\n\\n성과 (개인 기여도: 70%) :\\n\\n플랫폼 전체 기능 Python 구현 및 내재화 완료\\n\\n플랫폼 오퍼레이션 전반에 대한 유지, 보수, 관리 및 분석 프로젝트를 수행함\\n\\n공공 데이터베이스로부터 23개 암종에 걸쳐 약 24,000개 샘플 데이터를 수집하여 대규모 분석 기반을 구축함\\n\\n수집한 샘플 전사체 데이터 정규화 및 정량적 유전자 발현량으로 변환하여 분석 가능한 데이터셋을 구축함\\n\\n플랫폼 대표 성공 사례\\n\\n중국 ㅇㅇㅇ사 개발 신규 항암제의 주 적응증 예측 성공 및 잠재 적응증 후보 추가 제안\\n\\n국내 유명 제약사와 협업 – 후보물질 개발 단계 참여, 개발 물질에 대한 적응증 후보 도출, 최종 약물 타깃으로 선정되어 개발 지속\\n\\n상장 기술평가 항목 3가지 플랫폼 중 하나로서 회사 상장에 기여\\n\\nPython · R · pandas · NumPy · scikit-learn · Bioinformatics tools (survival, survminer, lifelines, GSEA, etc.) · Visualization packages (Matplotlib, seaborn, Plotly, etc.)\\n\\n\\n\\n<프로젝트2 : 항암제 플랫폼 고도화 프로젝트>\\n\\n1) 담당 업무 및 역할 : 암종 별, 유전자 예후 예측 스코어링 알고리즘 고도화\\n\\n2) 프로젝트 상세 내용\\n\\n프로젝트 기간 : 2024.01–2024.06 (6개월)'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\tmpedt4i9vn', 'chunk_id': '2'}, page_content='1) 담당 업무 및 역할 : 암종 별, 유전자 예후 예측 스코어링 알고리즘 고도화\\n\\n2) 프로젝트 상세 내용\\n\\n프로젝트 기간 : 2024.01–2024.06 (6개월)\\n\\n목표 및 역할 : 23개 암종에 대한 유전자 별 예후 점수 스코어링 알고리즘 속도 2배 (100%) 이상 개선\\n\\n성과 (개인 기여도: 100%) : \\n\\n코어 스코어링 알고리즘 전면 수정\\n\\n알고리즘 스코어링 속도 1100% 향상\\n\\n알고리즘 개선 후 소요시간: 최대 1주일 (기존 알고리즘: 최소 3개월)\\n\\nPython · pandas · NumPy · scikit-learn · Bioinformatics tools (lifelines, GSEA, etc.) · Visualization packages (Matplotlib, seaborn, Plotly, etc.)\\n\\n\\n\\n<프로젝트3 : 엑체생검을 통한 암 조기 진단 모델 개발>\\n\\n1) 담당 업무 및 역할 : \\n\\ncfDNA, ctDNA 데이터 수집, 관리, 분석 \\n\\n메타데이터 관리 \\n\\n암 진단 딥러닝 모델 개발\\n\\n2) 프로젝트 상세 내용\\n\\n프로젝트 기간 : 2025.05–2025.11 (6개월)\\n\\n목표 및 역할 : 혈액 cfDNA, ctDNA 데이터 수집 및 활용, 암 여부 예측 딥러닝 모델 개발\\n\\n성과 (개인 기여도: 20%) : \\n\\n모델 훈련을 위한 데이터 수집 (3000샘플 이상)\\n\\n암 진단 CNN모델 개발 (Recall: 0.69, Precision: 0.64, ROC-AUC: 0.71)\\n\\nPython · pandas · NumPy · PyTorch, TensorFlow · scikit-learn · Bioinformatics tools (GSEA, Bismark, Metilene) · Visualization packages (Matplotlib, seaborn, Plotly, etc.)\\n\\n\\n\\n<프로젝트4 : 신규 COVID-19 치료 후보 약물 탐색 및 연구 결과 논문 게재>'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\tmpedt4i9vn', 'chunk_id': '3'}, page_content='<프로젝트4 : 신규 COVID-19 치료 후보 약물 탐색 및 연구 결과 논문 게재>\\n\\n1) 담당 업무 및 역할 : Dry-lab 연구, 내용 정리 및 논문 작성\\n\\n2) 프로젝트 상세 내용\\n\\n프로젝트 기간 : 2024.01–present (21개월)\\n\\n목표 및 역할 : 공동 1저자(주 저자), 논문 Submission (2024.05.09)\\n\\n성과 (개인 기여도: 40%) : \\n\\n사내 플랫폼 활용 COVID-19 치료 유력 후보물질 발굴 성공\\n\\n연구 결과 Scientific reports 저널에 제출\\n\\n2025년 9월 현재까지 revision 진행중 (in press)\\n\\nPython · pandas · NumPy · scikit-learn · GSEA · Visualization packages (Matplotlib, seaborn)\\n\\n\\n\\n[자격증]\\n\\nOPIc(영어) · AL (Advanced Low) · 2024.09.20\\n\\n데이터 분석 준전문가 (ADsP) · 2025.09.05\\n\\n\\n\\n[교육 수료]\\n\\nGoogle Advanced Data Analytics – Foundations of Data Science\\n\\nGoogle Advanced Data Analytics – Get Started with Python\\n\\nGoogle Advanced Data Analytics – Go Beyond the Numbers: Translate Data into Insights\\n\\nGoogle Advanced Data Analytics – The Power of Statistics\\n\\nGoogle Advanced Data Analytics – Regression Analysis: Simplify Complex Data Relationships\\n\\nGoogle Advanced Data Analytics – The Nuts and Bolts of Machine Learning'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\tmpedt4i9vn', 'chunk_id': '4'}, page_content='Google Advanced Data Analytics – The Nuts and Bolts of Machine Learning\\n\\nPalantir Foundry – Speedrun: Your First End-to-End Workflow\\n\\nPalantir Foundry – Speedrun: Your First AIP workflow\\n\\nPalantir Foundry – Deep Dive: Building Your First Pipeline\\n\\nPalantir Foundry – Deep Dive: Creating Your First Ontology\\n\\nPalantir Foundry – Deep Dive: Transforming Your Data with Code Repositories\\n\\nPalantir Foundry – Deep Dive: Building Your First Application\\n\\nPalantir Foundry – Deep Dive: Data Analysis in Contour\\n\\n모두의 연구소 – 데이터 사이언티스트 과정 Bootcamp (2025.06~현재 진행중)\\n\\n\\n\\n[학력]\\n\\n학위\\n\\n전공\\n\\n학교\\n\\n기간\\n\\n박사 (수료)\\n\\n생명화학공학\\n\\n한국과학기술원 (KAIST, 대전)\\n\\n2015.08 – 2020.09\\n\\n석사 (졸업)\\n\\n생명화학공학\\n\\n한국과학기술원 (KAIST, 대전)\\n\\n2015.08 – 2020.09\\n\\n학사 (졸업)\\n\\n생명과학\\n\\nVirginia Tech (미국)\\n\\n2011.08\\n\\n\\n\\n[기타]\\n\\nGithub Personal Project Repository\\n\\n(https://github.com/milkpotato1000/modulabs_projects)\\n\\nData Science & AI학습 블로그 \\n\\n(https://velog.io/@milkpotato1000/posts)\\n\\n\\n\\n\\n\\n     2')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs_with_chunk_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b655b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "# Chroma 벡터스토어 생성 (persist_directory 지정하면 재사용 가능)\n",
    "persist_dir = './chroma_resume_db'\n",
    "if not os.path.exists(persist_dir):\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "# 새 vectorstore 생성\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_docs_with_chunk_id,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892ff3d-0588-4085-ac2d-1d37669a4b43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Retriever 구성 (검색 옵션: simple similarity or MMR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06a4aa80-ccdb-43da-af34-d25f347c16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever 설정\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # 다양성을 고려한 검색\n",
    "    search_kwargs={\n",
    "        \"k\": 5,       # 최종 반환할 문서 수\n",
    "        \"fetch_k\": 20  # 검색 후보로 고려할 청크 수\n",
    "    }\n",
    ")\n",
    "\n",
    "query = \"내가 다녔던 회사를 알아맞춰봐\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# 청크 내용을 하나로 합쳐서 GPT-40-mini에 전달\n",
    "context = \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cecfacb-ba53-469c-8142-84ce57500719",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. 검색 → LLM에 컨텍스트 전달 → 응답 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8596d11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신이 다녔던 회사는 \"(주)온코크로스\"입니다. 이 회사에서 AI 연구소의 주임연구원으로 2021년 1월부터 2024년 12월까지 근무했으며, 다양한 데이터 사이언스 및 AI 관련 프로젝트에 참여하였다고 기술서에서 명시되어 있습니다.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o-mini',  # 모델은 여기서 지정\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': '다음 정보를 기반으로 사용자가 다녔던 회사를 알려주세요.'},\n",
    "        {'role': 'user', 'content': context + '\\n\\n질문: ' + query}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50842ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CUahczI3IDcOLsUT5TUNwbPxbmeWY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='사용자가 다녔던 회사는 \"(주)온코크로스\"입니다.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1761406960, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=19, prompt_tokens=1987, total_tokens=2006, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ba1ac-cef1-4328-ad85-3f46884d3a12",
   "metadata": {},
   "source": [
    "## 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b056d123-fd7a-491f-adab-72b03f5d1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9246e3ee-31be-4887-aab5-4fbceba3cbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 00:41:11.390 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-26 00:41:11.407 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\user\\miniconda3\\envs\\ragenv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-10-26 00:41:11.407 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-26 00:41:11.407 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-26 00:41:11.408 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-26 00:41:11.408 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-26 00:41:11.408 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-26 00:41:11.409 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-26 00:41:11.409 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-26 00:41:11.409 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# 1. API KEY\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "st.title(\"💬 자기소개서 작성 도우미\")\n",
    "\n",
    "# 2. 경력기술서 업로드\n",
    "uploaded_file = st.file_uploader(\"경력기술서 업로드 (.pdf/.docx)\", type=['pdf','docx'])\n",
    "if uploaded_file:\n",
    "    if uploaded_file.name.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(uploaded_file)\n",
    "    else:\n",
    "        loader = Docx2txtLoader(uploaded_file)\n",
    "    docs = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99846a3c-4fb0-4476-8755-2f202918df10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
